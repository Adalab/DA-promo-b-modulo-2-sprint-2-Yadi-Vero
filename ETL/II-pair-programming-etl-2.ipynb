{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell # Nos permite mostar más de una salida por celda\n",
    "InteractiveShell.ast_node_interactivity = \"all\" # Nos permite mostar más de una salida por celda\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "pd.options.display.max_columns=None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tendréis que usar el csv attacks_limpieza_completa que tenéis adjunto abajo.\n",
    "En la lección de hoy aprendimos como transformar nuestros datos para que estén preparados para almacearlos en una BBDD. En este momento tenemos dos fuentes de datos:\n",
    "\n",
    "1.- El csv con los ataques de tiburones que hemos estado limpiando hasta ahora, el que os hemos adjuntado (attacks_limpieza_completa). Sentiros libres de usar vuestros propios csv en caso de que queráis.\n",
    "\n",
    "2.- El csv con los datos climáticos de los principales paises que tienen ataques de tiburones, el que creamos en el pair programming de ayer.\n",
    "\n",
    "El objetivo de la sesión de hoy será juntar en un único csv la información de ambas fuentes. Para ello:\n",
    "\n",
    "- Cargaremos los dos ficheros de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack=pd.read_csv('../ETL/datos/attacks_limpieza_completa.csv', index_col=0)\n",
    "df=pd.read_csv('../ETL/datos/df_API_paises.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Del dataframe de los ataques nos quedaremos solo con las filas de los países que seleccionamos en la lección de ayer:\n",
    "    - USA\n",
    "    - Australia\n",
    "    - New Zealand\n",
    "    - South Africa\n",
    "    - Papua New Guinea\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_attack=attack[(attack[\"country\"]==\"usa\") | (attack[\"country\"]==\"australia\")|\\\n",
    "          (attack[\"country\"]==\"south africa\") | (attack[\"country\"]==\"papua new guinea\")|\\\n",
    "          (attack[\"country\"]==\"new zealand\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14736/4288119105.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_attack.rename(columns={'country':'id_paises'}, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "df_attack.rename(columns={'country':'id_paises'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14736/3667869154.py:1: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  df_attacks2=df_attack.groupby('id_paises')['year','age','latitud','longitud','fatal_N'].agg(['mean'])\n"
     ]
    }
   ],
   "source": [
    "df_attacks2=df_attack.groupby('id_paises')['year','age','latitud','longitud','fatal_N'].agg(['mean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Del dataframe de los datos climáticos seleccionaremos todas las columnas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cuando ya tengamos todos los datos deseados juntaremos los dos csv.\n",
    "    - Para hacer esta unión tendremos que hacer un groupby en la tabla de clima para sacar una media de las medidas climáticas por país."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Antes de hacer el groupby si nos fijamos tenemos dos columnas rh_profile y wind_profile cuya información es una lista de diccionarios. Si intentamos hacer la media de eso no nos dará un valor real. A este problema ya nos enfrentamos en la clase invertida de ETL-2, donde teníais un Bonus para desempaquetar esta información. En caso de que en aquel ejercicio no lo consigierais os dejamos por aquí una posible solución que nos permite separar esa información en distintas columnas. Os dejamos el código documentado. ⚠️ Os recomendamos que vayáis desgranando el código y viendo lo que nos devuelve cada línea de código para entenderlo mejor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['rh_profile']=df['rh_profile'].apply(ast.literal_eval)\n",
    "rh = df['rh_profile'].apply(pd.Series)\n",
    "\n",
    "for i in range(len(rh.columns)):\n",
    "    rh_col0=rh[i].apply(pd.Series)\n",
    "    nombre=\"rh\"+ rh_col0[\"layer\"][115]\n",
    "    valores= list(rh_col0[\"rh\"])\n",
    "    df.insert(i, nombre, valores)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop(['rh_profile'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wind direction\n",
    "\n",
    "df['wind_profile']=df['wind_profile'].apply(ast.literal_eval)\n",
    "wind = df['wind_profile'].apply(pd.Series)\n",
    "\n",
    "for i in range(len(wind.columns)):\n",
    "    wind_col0=wind[i].apply(pd.Series)\n",
    "    nombre=\"wind_\"+ wind_col0[\"layer\"][1]\n",
    "    valores= list(wind_col0[\"direction\"])\n",
    "    df.insert(i, nombre, valores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop(['wind_profile'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14736/1401032147.py:1: FutureWarning: ['prec_type'] did not aggregate successfully. If any error is raised this will raise in a future version of pandas. Drop these columns/ops to avoid this warning.\n",
      "  df2=df.groupby('paises').agg(['mean'])\n"
     ]
    }
   ],
   "source": [
    "df2=df.groupby('paises').agg(['mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df2.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.rename(columns={'paises':'id_paises'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minuscula(col):\n",
    "    try:\n",
    "        col=col.lower()\n",
    "        return col\n",
    "    except:\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[\"id_paises\"]=df2[\"id_paises\"].apply(minuscula)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_final=pd.merge(left=df2, right=df_attacks2, how='left', left_on='id_paises', right_on='id_paises')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Guardar los resultados obtenidos en un csv que usaremos en próximos ejercicios de pair programming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_final.to_csv('datos/merge_clima_ataques.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creamos las funciones de nuestro código para automatizar todo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiar_attack (df):\n",
    "    df_attack=df[(df[\"country\"]==\"usa\") | (df[\"country\"]==\"australia\")|\\\n",
    "          (df[\"country\"]==\"south africa\") | (df[\"country\"]==\"papua new guinea\")|\\\n",
    "          (df[\"country\"]==\"new zealand\")]\n",
    "    df_attack.rename(columns={'country':'id_paises'}, inplace=True)\n",
    "    df_final=df_attack.groupby('id_paises')['year','age','latitud','longitud','fatal_N'].agg(['mean'])\n",
    "    df_final=df_final.reset_index()\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpieza_clima(df):\n",
    "    #Limpiamos la columna de rh_profile primero a series y luego a columnas en el bbucle for\n",
    "    df['rh_profile']=df['rh_profile'].apply(ast.literal_eval)\n",
    "    rh = df['rh_profile'].apply(pd.Series)\n",
    "\n",
    "    for i in range(len(rh.columns)):\n",
    "        rh_col0=rh[i].apply(pd.Series)\n",
    "        nombre=\"rh\"+ rh_col0[\"layer\"][115]\n",
    "        valores= list(rh_col0[\"rh\"])\n",
    "        df.insert(i, nombre, valores)\n",
    "    #Eliminamos la columna que ya no necesitamos\n",
    "    df=df.drop(['rh_profile'], axis=1)\n",
    "    \n",
    "    #Hacemos lo mismo para la columnas wind_profile\n",
    "    df['wind_profile']=df['wind_profile'].apply(ast.literal_eval)\n",
    "    wind = df['wind_profile'].apply(pd.Series)\n",
    "\n",
    "    for i in range(len(wind.columns)):\n",
    "        wind_col0=wind[i].apply(pd.Series)\n",
    "        nombre=\"wind_\"+ wind_col0[\"layer\"][1]\n",
    "        valores= list(wind_col0[\"direction\"])\n",
    "        df.insert(i, nombre, valores)\n",
    "        \n",
    "    df=df.drop(['wind_profile'], axis=1)\n",
    "    \n",
    "    df2=df.groupby('paises').agg(['mean'])\n",
    "    df2=df2.reset_index()\n",
    "    df2.rename(columns={'paises':'id_paises'}, inplace=True)\n",
    "    def minuscula(col):\n",
    "        try:\n",
    "            col=col.lower()\n",
    "            return col\n",
    "        except:\n",
    "            pass\n",
    "    df2[\"id_paises\"]=df2[\"id_paises\"].apply(minuscula)\n",
    "\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(df_left,df_rigth): #Esta función está persanda para hacer un left. Para que se tenga en cuenta\n",
    "    merge_final=pd.merge(left=df_left, right=df_rigth, how='left', left_on='id_paises', right_on='id_paises')\n",
    "    return merge_final"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c077ba4c986d0c674841b1bde68a642c04a6e440695030e9d779c08f2cbfb9f4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
